<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Задача 14</title>
</head>
<body>
    <div class="all">
        <div class="info">   
            <h1>
                Сравнение открытых OLAP-систем <br>
                Big Data: ClickHouse, Druid и Pinot
            </h1>
            <p>
                Оригинал - https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-<br>
                for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7
            </p>
            <p>
                <a href="https://clickhouse.tech/">ClickHouse</a>, <a href="https://druid.apache.org/">Druid</a> и <a href="https://github.com/apache/incubator-pinot">Pinot</a> три открытых хранилища данных, которые позволяют <br>
                выполнять аналитические запросы на больших объемах данных с интерактивными <br>
                задержками. Эта статья - перевод <a href="https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7">подробного сравнения</a>, выполненного Романом <br>
                Левентовым.
            </p>
            <br>
        </div>
        <div class="Источник информации">
            <h2>Источники информации</h2>
            <p>
                Подробности реализации <b>ClickHouse</b> стали мне известны от <a href="https://github.com/ztlpn">Алексея Зателепина</a>,<br>
                одного из <b>ключевых разработчиков проекта</b>. Доступная на английском <br>
                документация достаточно скудна – наилучшим источником информации служат <br>
                последние четыре секции <a href="https://clickhouse.tech/docs/en/development/architecture.html">данной страницы документации</a><br>
            </p>
            <p>
                <b>Я сам участвую в развитии Druid</b>, но у меня нет личной заинтересованности в этой <br>
                системе - по правде говоря, скорее всего в ближайшее время я перестану заниматься <br>
                её разработкой. Поэтому читатели могут рассчитывать на отсутствие какой-либо <br>
                предвзятости. <br>
            </p>
            <p>
                Всё, что я буду далее писать про <b>Pinot</b>, основывается на странице <a href="https://github.com/apache/incubator-pinot/wiki">Архитектура в вики<br> Pinot</a>,
                а также на других страницах вики в разделе “Проектная документация”. <br>
                Последний раз они обновлялись в июне 2017 года - больше, чем полгода назад. <br>
            </p>
            <p>
                Рецензентами оригинальной статьи стали Алексей Зателепин и <a href="https://github.com/ludv1x">Виталий Людвиченко</a><br>
                (разработчики ClickHouse), <a href="https://github.com/gianm">Жан Мерлино</a> (самый активный разработчик Druid), <a href="https://github.com/kishoreg">Кишор <br> Гопалакришна</a>
                (архитектор Pinot) и <a href="https://github.com/jfim">Жан-Француа Им</a> (разработчик Pinot). Мы <br>
                присоединяемся к благодарности автора и полагаем, что это многократно повышает <br>
                авторитетность статьи. 
            </p>
            <p>
                <b>Предупреждение</b>: статья достаточно большая, поэтому вполне возможно вы захотите <br>
                ограничиться прочтением раздела “Заключение” в конце.
            </p>
        </div>
        <div class="Сходства между системами">
            <h2>Сходства между системами</h2>
            <div class="Связанные данные и вычисления ">
                <h3>Связанные данные и вычисления</h3>
                <b>На фундаментальном уровне, ClickHouse, Druid и Pinot похожи</b>, поскольку они <br>
                хранят данные и выполняют обработку запросов на одних и тех же узлах, уходя от <br>
                “разъединенной” архитектуры BigQuery. Недавно я уже описывал несколько <br>
                наследственных проблем со связанной архитектурой в случае Druid [<a href="https://leventov.medium.com/the-problems-with-druid-at-large-scale-and-high-load-part-1-714d475e84c9">1</a>, <a href="https://leventov.medium.com/the-challenges-of-running-druid-at-large-scale-and-future-directions-part-2-ef594ce298f2">2</a>]. Открытого <br>
                эквивалента для BigQuery на данный момент не существует (за исключением, разве <br>
                что, <a href="http://drill.apache.org/">Drill</a>?) Возможным подходам к построению подобных открытых систем посвящена 
                <a href="https://leventov.medium.com/design-of-a-cost-efficient-time-series-store-for-big-data-88c5dc41af8e">другая статье в моем блоге.</a>
            </div>
            <div class="Отличия от Big Data SQL-систем:">
                <h3>Отличия от Big Data SQL-систем: индексы и статическое <br> распределение данных</h3>
                <p>
                    Рассматриваемые в этой статье системы <b>выполняют запросы быстрее</b>, чем <br>
                    системы Big Data из семейства класса SQL-on-Hadoop: Hive, Impala, Presto и Spark <br>
                    даже когда последние получают доступ к данным, хранящимся в колоночном формате <br>
                    - к примеру, Parquet или Kudu. Это происходит потому, что в ClickHouse, Druid и Pinot: <br>
                </p>
                <ul>
                    <li>
                        <p> Имеется <b>свой собственный формат для хранения данных с индексами</b>, и<br>
                            и они тесно интегрированы с движками обработки запросов. Системы класса <br>
                            SQL-on-Hadoop обычно можно назвать агностиками относительно форматов<br>
                            данных и поэтому они менее “навязчивы” в бэкендах Big Data. <br>
                        </p>
                    </li>
                    <li>
                        <p>
                            <b>Данные распределены относительно “статично”</b> между узлами, и при <br>
                            распределенном выполнении запроса это можно использовать. Обратная <br>
                            сторона медали при этом в том, что ClickHouse, Druid и Pinot <b>не поддерживают</b> <br>
                            <b>запросы, которые требуют перемещения большого количества данных</b> <br>
                            между узлами - к примеру, join между двумя большими таблицами. <br>
                        </p>
                    </li>
                </ul>
            </div>
            <div class="Отсутствие точечных обновлений и удалений">
                <h3>Отсутствие точечных обновлений и удалений</h3>
                <p>
                    Находясь на другой стороне спектра баз данных, ClickHouse, Druid и Pinot <b>не</b> <br>
                    <b>поддерживают точечные обновления и удаления</b>, в противоположность <br>
                    колоночным системам вроде Kudu, InfluxDB и Vertica (?). Это даёт ClickHouse, Druid и<br>
                    Pinot возможность производить более эффективное колоночное сжатие и более<br>
                    агрессивные индексы, что означает <b>большую эффективность использования</b><br>
                    <b>ресурсов</b> и быстрое выполнение запросов. <br>
                </p>
                <p>
                    Разработчики ClickHouse в Yandex планируют начать поддерживать <a href="#">обновления и <br> удаления в будущем,</a>
                    но я не уверен, будут ли это “настоящие” точечные запросы или <br>
                    обновления/удаления диапазонов данных. <br>
                </p>
            </div>
            <div class="Поглощение в стиле Big Data">
                <h3>Поглощение в стиле Big Data</h3>
                <p>
                    Все три системы поддерживают потоковое поглощение данных из Kafka. Druid и Pinot <br>
                    поддерживают потоковую передачу данных стриминг в <a href="https://en.wikipedia.org/wiki/Lambda_architecture">Лямбда-стиле</a> и пакетное<br>
                    поглощение одних и тех же данных. ClickHouse поддерживает пакетные вставки<br>
                    напрямую, поэтому ему не требуется отдельная система пакетного поглощения <br>
                    подобная той, что используется в Druid и Pinot. Если вас интересуют подробности, то<br>
                    их вы сможете найти далее. <br> 
                </p>
            </div>
            <div class="Проверено на крупном масштабе">
                <h3>Проверено на крупном масштабе</h3>
                <p>
                    Все три системы проверены на работоспособность в крупных масштабах: в <br>
                    <a href="https://clickhouse.tech/blog/en/2016/evolution-of-data-structures-in-yandex-metrica/">Yandex.Metrica работает кластер ClickHouse</a>, состоящий из примерно десятка тысяч <br>
                    ядер CPU. В Metamarkets используется <a href="https://leventov.medium.com/the-problems-with-druid-at-large-scale-and-high-load-part-1-714d475e84c9">кластер Druid аналогичного размера</a>. Один <br>
                    кластер Pinot в LinkedIn включает в себя "<a href="https://github.com/apache/incubator-pinot/issues/3#issuecomment-228497765">тысячи машин</a>" <br>
                </p>
            </div>
            <div class="Незрелость">
                <h3>Незрелость</h3>
                <p>
                    Все рассматриваемые в статье системы являются <b>незрелыми по меркам открытых </b><br>
                    <b>enterprise-систем Big Data</b>. Однако, скорее всего они незрелы не более, чем <br>
                    среднестатистическая открытая система Big Data - но это совсем другая история. <br>
                    ClickHouse, Druid и Pinot недостает некоторых очевидных оптимизаций и <br>
                    функциональности, и они кишат багами (насчет ClickHouse и Pinot я не уверен на все <br>
                    100%, но не вижу причин, по которым они в этом плане были бы лучше Druid). <br>
                </p>
                <p>
                    Это плавно подводит нас к следующему важному разделу. 
                </p>
            </div>
        </div>
        <div class="Про сравнение производительности и выбор системы">
            <h2>Про сравнение производительности и выбор системы</h2>
            <div>   
                <p>
                    Я регулярно вижу в сети, как некоторые проводят сравнения систем больших данных:<br>
                    они берут набор своих данных, каким-либо образом “скармливают” его оцениваемой<br>
                    системе, а затем немедленно пытаются измерить производительность - сколько<br>
                    памяти или дискового пространства было занято, и насколько быстро выполнялись<br>
                    запросы. Причем понимание того, как устроены изнутри испытываемые ими системы, у<br>
                    них отсутствует. Затем, используя лишь подобные специфичные данные о<br>
                    производительности - иногда вместе со списками функциональности, которая им<br>
                    нужна и которая есть в системе на настоящий момент, - они в итоге делают свой<br>
                    выбор или, что еще хуже, выбирают написать свою собственную, “лучшую” систему с<br>
                    нуля.<br> 
                </p>
                <p>
                    Такой подход мне кажется неправильным, по крайней мере он неприменим в<br>
                    отношении открытых OLAP-систем для Big Data. Задача создания системы Bid Data<br>
                    OLAP, которая смогла бы работать эффективно в большинстве сценариев<br>
                    использования и содержала бы все необходимые функции настолько велика, что я<br>
                    оцениваю ее реализацию как минимум в <b>100 человеко-лет</b>.100 человеко-лет
                </p>
                <p>
                    На сегодня, ClickHouse, Druid и Pinot оптимизированы только для конкретных<br>
                    сценариев использования, которые требуются их разработчиком - и содержат по<br>
                    большей части лишь те функции, в которых нуждаются сами разработчики. Я могу<br>
                    гарантировать, что ваш случай обязательно “упрется” в те узкие места, с которыми<br>
                    разработчики рассматриваемых OLAP-систем еще не сталкивались - или же в те<br>
                    места, что их не интересуют.<br>
                </p>
                <p>
                    Не говоря уже о том, что упомянутый выше подход “забросить данные в систему, о<br>
                    которой вы ничего не знаете, и затем измерить её эффективность” весьма вероятно<br>
                    даст искаженный результат из-за серьезных “узких” мест, которые на самом деле<br>
                    могли бы быть исправлены <b>простым изменением конфигурации</b>, схемы данных или<br>
                    другим построением запроса.<br>
                </p>
            </div>
            <div class="CloudFlare: ClickHouse против Druid">
                <h3>CloudFlare: ClickHouse против Druid</h3>
                <p>
                    Одним таким примером, хорошо иллюстрирующим описанную выше проблему,<br>
                    является пост Марека Вавруша о <a href="https://blog.cloudflare.com/how-cloudflare-analyzes-1m-dns-queries-per-second/#comment-3302778860">выборе между ClickHouse и Druid в Cloudflare</a>. Им<br>
                    потребовалось 4 сервера ClickHouse (которые со временем превратились в 9), и по их<br>
                    оценкам, для разворачивания аналогичной установки Druid им бы потребовались<br>
                    “сотни узлов”. Пусть Марек и признает, что сравнение является нечестным,<br>
                    поскольку Druid недостаёт “сортировки по первичному ключу”, он возможно даже не<br>
                    осознает, что достичь примерно того же самого эффекта в Druid возможно просто<br>
                   <a href="http://druid.io/docs/0.11.0/ingestion/index.html"> установив правильный порядок измерений в “ingestion spec”</a> и произведя простую<br>
                    подготовку данных: обрезать значение колонки __time в Druid до некоей грубой<br>
                    детализации (к примеру, один час) и опционально добавить другую “длинно-типовую”<br>
                    колонку “precise_time”, если для некоторых запросов требуются более точные<br>
                    временные рамки. Да, это хак, но, как мы только что выяснили, и в Druid можно<br>
                    сортировать данные по какому-либо измерению перед __time, и это достаточно<br>
                    просто реализовать.<br>
                </p>
                <p>
                    Впрочем, я не стану спорить с их итоговым решением выбрать ClickHouse, поскольку<br>
                    на масштабе примерно в 10 узлов и для их нужд ClickHouse мне тоже кажется лучшим<br>
                    выбором, чем Druid. Но сделанное ими заключение о том, что ClickHouse как минимум<br>
                    на порядок эффективнее (по меркам стоимости инфраструктуры), чем Druid - это<br>
                    серьезное заблуждение. На самом деле, из рассматриваемых нами сегодня систем,<br>
                    <b>Druid предлагает наилучшую возможность для реально дешевых установок</b><br>
                    (смотрите раздел “Уровни узлов обработки запросов в Druid ” ниже).
                </p>
                <blockquote style="background-color: yellow; display: inline-block;">
                    Когда вы выбираете систему OLAP Big Data, не сравнивайте то, насколько они<br>
                    сейчас хорошо подходят для вашего случая. Сейчас они все субоптимальны.<br>
                    Вместо этого, сравните, насколько быстро ваша компания способна заставить<br>
                    двигаться эти системы в том направлении, которое нужно именно вам.
                </blockquote>
                <p>
                    В силу своей фундаментальной архитектурной схожести, ClickHouse, Druid и Pinot<br>
                    имеют примерно один и тот же “предел” эффективности и оптимизации<br>
                    производительности. Здесь нет “волшебной таблетки”, которая позволила бы какой-<br>
                    либо из этих систем быть быстрее, чем остальные. Не позволяйте запутать себя тем<br>
                    фактом, что <i>в своем текущем состоянии</i> системы показывают себя очень по-разному<br>
                    в различных бенчмарках. 
                </p>
                <p>
                    Допустим, Druid не поддерживает “сортировку по первичному ключу” настолько<br>
                    хорошо, насколько это умеет ClickHouse - а ClickHouse в свою очередь не<br>
                    поддерживает “инвертированные индексы” столь же хорошо, как Druid, что дает<br>
                    данным системам преимущества с той или иной нагрузкой. <b>Упущенные оптимизации</b><br>
                    <b>могут быть реализованы в выбранной системе при помощи не таких уж и</b><br>
                    <b>больших усилий</b>, если у вас есть намерение и возможность решиться на подобный<br>
                    шаг
                </p>
                <ul>
                    <li>
                        <p>
                            В вашей организации должны быть инженеры, способные прочитать, понять и<br>
                            модифицировать исходный код выбранной системы, к тому же у них должно<br>
                            быть на это время. Заметьте, что ClickHouse написан на C++, а Druid и Pinot — <br>
                            на Java.<br>
                        </p>
                    </li>
                    <li>
                        <p>
                            Или же ваша организация должна подписать контракт с компанией, которая<br>
                            оказывает поддержку выбранной системы. Это будут Altinity для ClickHouse,<br>
                            Imply и Hortonworks для Druid. Для Pinot таких компаний в данный момент нет. 
                        </p>
                    </li>
                </ul>
                <p>Другие сведения о разработке систем, которые вам стоит принять во внимание:</p>
                <ul>
                    <li>
                        <p>
                            Авторы ClickHouse, работающие в Yandex, утверждают, что они тратят 50%<br>
                            своего времени на создание функциональности, которая требуется им внутри<br>
                            компании, и другие 50% уходят на функции, который набирают большинство<br>
                            “голосов сообщества”. Однако, чтобы вы получили от этого факта<br>
                            преимущество, требуется, чтобы <b>функции, которые нужны вам, были и</b><br>
                            <b>наиболее востребованы сообществом</b> ClickHouse.
                        </p>
                    </li>
                    <li>
                        <p>
                            Разработчики Druid из Imply мотивированы работать над широко<br>
                            используемыми функциями, поскольку это позволит им максимально увеличить<br>
                            объем охвата своего бизнеса в будущем.
                        </p>
                    </li>
                    <li>
                        <p>
                            Процесс разработки Druid сильно напоминает <a href="https://community.apache.org/apache-way/apache-project-maturity-model.html">модель Apache</a>, когда ПО<br>
                            несколько лет разрабатывается несколькими компаниями, у каждой из который<br>
                            достаточно своеобразные и различные приоритеты, и среди них нет ведущей<br>
                            компании. ClickHouse и Pinot пока еще далеки от подобного этапа, поскольку<br>
                            ими занимаются соответственно лишь Yandex и Linkedin. Сторонний вклад в<br>
                            развитие Druid имеет минимальный шанс быть отклоненным в силу того, что он<br>
                            расходится с видением основного разработчика - ведь <b>в Druid нет “основной”</b><br>
                            <b>компании-разработчика</b>.
                        </p>
                    </li>
                    <li>
                        <p>
                            Druid поддерживает “API разработчика”, который позволяет привносить<br>
                            собственные типы колонок, механизмы агрегации, возможные варианты для<br>
                            «глубокого хранения» и пр., причем все это вы можете держать в кодовой базе,<br>
                            отдельной от самого ядра Druid. Данное API документировано разработчиками<br>
                            Druid, и они следят за его совместимостью с предыдущими версиями. Однако,<br>
                            оно недостаточно “взрослое”, и ломается практически с каждым новым релизом<br>
                            Druid. Насколько мне известно, в ClickHouse и Pinot схожие API не<br>
                            поддерживаются.
                        </p>
                    </li>
                    <li>
                        <p>
                            Согласно Github, <b>над Pinot работает наибольшее число людей</b> - по всей<br>
                            видимости, лишь за прошлый год в Pinot было вложено <a href="https://github.com/apache/incubator-pinot/graphs/contributors?from=2017-01-24&to=2018-01-24&type=c">не менее 10 человеко-<br>лет</a>
                            . Для ClickHouse эта цифра составляет примерно 6 человеко-лет, а для<br>
                            Druid - 7. В теории, это должно означать, что Pinot улучшается быстрее всех<br>
                            остальных систем, которые мы рассматриваем.
                        </p>
                    </li>
                </ul>
                <p>
                    Архитектуры Druid и Pinot почти что идентичны друг другу, в то время как ClickHouse<br>
                    стоит слегка в стороне. Поэтому сначала мы сравним ClickHouse c “обобщенной”<br>
                    архитектурой Druid/Pinot, а затем обсудим мелкие различия между Druid и Pinot.
                </p>
            </div>
        </div>
        <div class="Различия между ClickHouse и Druid/Pinot">
            <h2>Различия между ClickHouse и Druid/Pinot</h2>
            <div class="Управление данными: Druid и Pinot">
                <h3>Управление данными: Druid и Pinot</h3>
                <p>
                    В Druid и Pinot, все данные в каждой “таблице” (как бы она не называлась в<br>
                    терминологии этих систем) разбиваются на указанное количество частей. По<br>
                    временой оси, данные обычно разделены с заданым интервалом. Затем эти части<br>
                    данных «запечатываются» индивидуально в самостоятельные автономные сущности,<br>
                    называемые «сегментами». Каждый сегмент включает в себя метаданные таблицы,<br>
                    сжатые столбчатые данные и индексы.
                </p>
                <p>
                    Сегменты хранятся в файловой системе хранилища «глубокого хранения» (например,<br>
                    HDFS) и могут быть загружены на узлы обработки запросов, но последние не отвечают<br>
                    за устойчивость сегментов, поэтому узлы обработки запросов могут быть заменены<br>
                    относительно свободно. <b> Сегменты не привязаны жестко к конкретным узлам</b> и<br>
                    могут быть загружены на те или другие узлы. Специальный выделенный сервер<br>
                    (который называется “координатором” в Druid и “контроллером” в Pinot, но я ниже<br>
                    обращаюсь к нему как к “мастеру”) отвечает за присвоение сегментов узлам, и<br>
                    перемещению сегментов между узлами, если потребуется.
                </p>
                <p>
                    Это не противоречит тому, что я отмечал выше, все три системы имеют статическое<br>
                    распределение данных между узлами, поскольку загрузки сегментов и их<br>
                    перемещения в Druid - и насколько я понимаю в Pinot - являются дорогими<br>
                    операциями и потому не выполняются для каждой отдельной очереди, а происходят<br>
                    обычно раз в несколько минут/часов/дней.
                </p>
                <p>
                    Метаданные сегментов хранятся в ZooKeeper - напрямую в случае Druid, и при<br>
                    помощи фреймворка <a href="http://helix.apache.org/">Helix</a> в Pinot. В Druid метаданные также хранятся в базе SQL, об<br>
                    этом будет подробнее в разделе “Различия между Druid и Pinot”.
                </p>
            </div>
            <div class="Управление данными: ClickHouse">
                <h3>Управление данными: ClickHouse</h3>
                <p>
                    В ClickHouse нет “сегментов”, содержащих данные, попадающие в конкретные<br>
                    временные диапазоны. В нем нет “глубокого хранения” для данных, узлы в кластере<br>
                    ClickHouse также отвечают и за обработку запросов, и за постоянство/устойчивость<br>
                    данных, хранящихся на них. Так что вам <b>не потребуется HDFS</b> или облачное<br>
                    хранилище данных вроде Amazon S3.
                </p>
                <p>
                    В ClickHouse имеются секционированные таблицы, состоящие из указанного набора<br>
                    узлов. Здесь нет “центральной власти” или сервера метаданных. Все узлы, между<br>
                    которыми разделена та или иная таблица, содержат полные, идентичные копии<br>
                    метаданных, включая адреса всех остальных узлов, на которых хранятся секции этой<br>
                    таблицы.
                </p>
                <p>
                    Метаданные секционированной таблицы включают “весы” узлов для распределения<br>
                    свежезаписываемых данных - к примеру, 40% данных должны идти на узел A, 30% на<br>
                    узел B и 30% на C. Обычно же распределение должно происходить равномерно,<br>
                    “перекоос”, как в этом примере, требуется только тогда, когда к секционированной<br>
                    таблице добавляется новый узел и нужно побыстрее заполнить его какими-либо<br>
                    данными. <b>Обновления этих “весов” должны выполняться вручную</b><br>
                    администраторами кластера ClickHouse, или же автоматизированной системой,<br>
                    построенной поверх ClickHouse.
                </p>
            </div>
            <div class="Управление данными: сравнение">
                <h3>Управление данными: сравнение</h3>
                <p>
                    Подход к управлению данными в ClickHouse проще, чем в Druid и Pinot: не требуется<br>
                    “глубокого хранилища”, всего один тип узлов, не требуется выделенного сервера для<br>
                    управления данными. Но подход ClickHouse приводит к некоторым трудностям, когда<br>
                    любая таблица данных вырастает настолько большой, что требуется ее разбиение<br>
                    между десятком или более узлов: коэффициент усиления запроса становится<br>
                    настолько же велик, насколько и фактор секционирования - даже для запросов,<br>
                    которые покрывают небольшой интервал данных:
                </p>
                <div class="img">
                    <img src="img/Рисунок1.png" alt="table">
                    <p><i>Компромисс распределения данных в ClickHouse</i></p>
                </div>
                <p>
                    В примере, показанном на изображении выше, данные таблицы распределены между<br>
                    тремя узлами в Druid/Pinot, но запрос по малому интервалу данных обычно<br>
                    затрагивает лишь два из них (до той поры, пока интервал не пересечет пограничный<br>
                    интервал сегмента). В ClickHouse, любые запросы будут вынуждены затронуть три<br>
                    узлв – если таблица сегментирована между тремя узлами. В данном примере разница<br>
                    не выглядит настолько существенно, однако представьте себе, что случится, если<br>
                    число узлов достигнет 100 – в то время как фактор сегментирования по-прежнему<br>
                    может быть равен, например, 10 в Druid/Pinot.
                </p>
                <p>
                    Чтобы смягчить эту проблему, самый большой кластер ClickHouse в Яндексе,<br>
                    состоящий из сотен узлов, в действительности разбит на многие «под-кластеры» с<br>
                    несколькими десятками узлов в каждом. Кластер ClickHouse используется в работе с<br>
                    аналитикой по веб-сайтам, и каждая точка данных имеет измерение «ID вебсайта».<br>
                    Существует жесткая привязка каждого ID сайта к конкретному под-кластеру, куда идут<br>
                    все данные для этого идентификатора сайта. Поверх кластера ClickHouse есть слой<br>
                    бизнес-логики, который управляет этим разделением данных при поглощении данных<br>
                    и выполнении запросов. К счастью, в их сценариях использования совсем немного<br>
                    запросов затрагивают несколько идентификаторов сайтов, и подобные запросы идут<br>
                    не от пользователей сервиса, поэтому у них нет жесткой привязки к реальному<br>
                    времени согласно соглашению об уровне услуг.
                </p>
                <p>
                    Другим недостатком подхода ClickHouse является то, что, когда кластер растет очень<br>
                    быстро, данные не могут перебалансироваться автоматически без участия человека,<br>
                    который вручную поменяет «веса» узлов в разбиваемой таблице.
                </p>
            </div>
            <div class="Уровни узлов обработки запросов в Druid">
                <h3>Уровни узлов обработки запросов в Druid</h3>
                <p>
                    Управление данными при помощи сегментов «проще себе представить» - эта<br>
                    концепция хорошо ложится на наши когнитивные способности. Сами сегменты можно<br>
                    перемещать между узлами относительно просто. Эта две причины позволили Druid<br>
                    реализовать “разделение на уровни” узлов, занимающихся обработкой запросов:<br>
                    старые данные автоматически перемещаются на сервера с относительно большими<br>
                    дисками, но меньшим количеством памяти и CPU, что позволяет <b>значительно</b><br>
                    <b>снизить стоимость большого рабочего кластера Druid</b> за счет замедления<br>
                    запросов к более старым данным.
                </p>
                <p>
                    Эта функция позволяет Metamarkets экономить сотни тысяч долларов расходов на<br>
                    инфраструктуру Druid каждый месяц - в противовес тому варианту, если бы<br>
                    использовался “плоский” кластер.
                </p>
                <img src="img/Рисунок2.png" alt="img">
                <p><i>Уровни узлов обработки запросов в Druid</i></p>
                <p>
                    Насколько мне известно, в ClickHouse и Pinot пока еще нет похожей функциональности<br>
                    - предполагается, что все узлы в их кластерах одинаковы.
                </p>
                <p>
                    В силу того, что архитектура Pinot весьма схожа с архитектурой Druid, как мне кажется,<br>
                    будет не слишком сложно добавить аналогичную функцию в Pinot. Тяжелее будет в<br>
                    случае с ClickHouse, поскольку для реализации данной функции крайне полезно<br>
                    использование концепта “сегментов”, однако это всё равно возможно.
                </p>
            </div>
            <div class="Репликация данных: Druid и Pinot">
                <h3>Репликация данных: Druid и Pinot</h3>
                <p>
                    Единицей репликации в Druid и Pinot является единичный сегмент. Сегменты <br>
                    реплицируются на уровне «глубокого хранения» (например, в три реплики на HDFS,<br>
                    или при помощи хранилища BLOB-объектов в Amazon S3), и на уровне обработки<br>
                    запросов: обычно и в Druid и в Pinot, каждый сегмент загружается на два различных<br>
                    узла. «Мастер»-сервер мониторит уровни репликации для каждого сегмента и<br>
                    загружает сегмент на какой-либо сервер, если фактор репликации падает ниже<br>
                    заданного уровня (например, если какой-либо из узлов перестаёт отвечать).
                </p>
            </div>
            <div class="Репликация данных: ClickHouse">
                <h3>Репликация данных: ClickHouse</h3>
                <p>
                    Единицей репликации в ClickHouse является секция таблицы на сервере (например,<br>
                    все данные из какой-либо таблицы, хранящиеся на сервере). Аналогично<br>
                    секционированию, репликация в ClickHouse является скорее «статической и<br>
                    конкретной», чем «в облачном стиле»: несколько серверов знают, что они являются<br>
                    репликами друг друга (для некоторой конкретной таблицы; в случае другой таблицы,<br>
                    конфигурация репликации может отличаться). Репликация предоставляет и<br>
                    устойчивость, и доступность запросов. Когда повреждается диск на одном узле,<br>
                    данные не теряются, поскольку они хранятся еще и на другом узле. Когда какой-либо<br>
                    узел временно недоступен, запросы могут быть перенаправлены на реплику. 
                </p>
                <p>
                    В самом большом кластере ClickHouse в Яндексе есть два одинаковых набора узлов в<br>
                    различных дата-центрах, и они спарены. В каждой паре узлы являются репликами<br>
                    друг друга (используется фактор репликации, равный двум), и они расположены в<br>
                    различных дата-центрах.
                </p>
                <p>
                    ClickHouse полагается на ZooKeeper для управления репликацией – поэтому, если вам<br>
                    не нужна репликация, то вам не нужен и ZooKeeper. Это означает, что ZooKeeper не<br>
                    потребуется и для ClickHouse, развернутого на одиночном узле.
                </p>
            </div>
            <div class="Поглощение данных: Druid и Pinot">
                <h3>Поглощение данных: Druid и Pinot</h3>
                <p>
                    В Druid и Pinot узлы обработки запросов специализируются на загрузке сегментов и<br>
                    обслуживают запросы к данным в сегментах; они не занимаются накоплением новых<br>
                    данных и производством новых сегментов.<br>
                    <br>
                    Когда таблица может обновляться с задержкой в час или более, сегменты создаются<br>
                    при помощи движков пакетной обработки – к примеру, Hadoop или Spark. И в Druid, и в<br>
                    Pinot есть первоклассная поддержка Hadoop из коробки. Существует <a href="https://github.com/metamx/druid-spark-batch">сторонний плагин<br>для поддержки индексации Druid в Spark</a><br>
                    , но в данный момент официально он не<br>
                    поддерживается. Насколько мне известно, в Pinot такого уровня поддержки Spark пока<br>
                    нет, то есть вы должны быть готовы разобраться источники), обслуживает запросы с<br>
                    недавними данными, создает сегменты в фоне и затем записывает их в “глубокое<br>
                    хранилище”.
                </p>
            </div>
            <div class="Поглощение данных: ClickHouse">
                <h3>Поглощение данных: ClickHouse</h3>
                <p>
                    Тот факт, что ClickHouse не требуется готовить “сегменты”, содержащие все данные и<br>
                    попадающие в заданные временные интервалы, позволяет строить более простую<br>
                    архитектуру поглощения данных. ClickHouse не требуется ни пакетный движок<br>
                    обработки вроде Hadoop, ни “реалтаймовые” узлы. Обычные узлы ClickHouse - те же<br>
                    самые, что занимаются хранением данных и обслуживают запросы к ним - напрямую<br>
                    принимают пакетные записи данных.<br>
                    <br>
                    Если таблица разбита на сегменты, то узел, который принимает пакетную запись<br>
                    (например, 10к строк) распределяет данные согласно “весам” (смотрите раздел ниже).<br>
                    Строки записываются одним пакетом, который формирует небольшое “множество”.<br>
                    Множество немедленно конвертируется в колоночный формат. На каждом узле<br>
                    ClickHouse работает фоновый процесс, который объединяет наборы строк в еще<br>
                    большие наборы. Документация ClickHouse сильно завязана на с интерфейсами Pinot<br>
                    и кодом, а затем самостоятельно написать код на Java/Scala, пусть это и не должно<br>
                    быть слишком сложно. (Впрочем, с момента публикации оригинальной статьи<br>
                    поддержка Spark в Pinot <a href="https://github.com/apache/incubator-pinot/pull/2388">была внесена контрибьютором</a>).<br>
                    <br>
                    Когда таблица должна обновляться в реальном времени, здесь приходит на помощь<br>
                    идея “реалтаймовых» узлов, которые делают три вещи: принимает новые данные из<br>
                    Kafka (Druid поддерживает и другие <br>
                    принцип, известный как “MergeTree”, и подчеркивает схожесть его работы с <a href="https://ru.wikipedia.org/wiki/LSM-%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE">LSM-<br>деревом</a>
                    , хотя меня это слегка смущает, поскольку данные не организованы в деревья<br>
                    - они лежат в плоском колончатом формате.
                </p>
                
            </div>
            <div class="Поглощение данных: сравнение">
                <h3>Поглощение данных: сравнение</h3>
                <p>
                    Поглощение данных в Druid и Pinot является “тяжелым”: оно состоит из нескольких<br>
                    различных сервисов, и управление ими - это тяжелый труд.<br>
                    <br>
                    Поглощение данных в ClickHouse гораздо проще (что компенсируется сложностью<br>
                    управления “историческими” данными - т.е. данными не в реальном времени), но и<br>
                    здесь есть один момент: вы должны иметь возможность собирать данные в пакеты до<br>
                    самого ClickHouse. Автоматическое поглощение и пакетный сбор данных из Kafka<br>
                    доступно “из коробки”, но если у вас используется другой источник данных в реальном<br>
                    времени (здесь подразумевается всё, что угодно, в диапазоне между инфраструктурой<br>
                    запросов, альтернативной Kafka, и стриминговых движков обработки, вплоть до<br>
                    различных HTTP-endpoint), то вам придется создать промежуточный сервис по сбору<br>
                    пакетов, или же внести код напрямую в ClickHouse.
                </p>
            </div>
            <div class="Выполнение запроса"> 
                <h3>Выполнение запроса</h3>
                <p>
                    В <b>Druid и Pinot</b> имеется отдельный слой узлов, называемых <i>“брокерами”</i>, которые<br>
                    принимают все запросы к системе. Они определяют, к каким “историческим”<br>
                    (содержащим данные не в реальном времени) узлам обработки запросов должны быть<br>
                    отправлены подзапросы, основываясь на отображении сегментов в узлы, в которых<br>
                    сегменты загружаются. Брокеры хранят информацию об отображении в памяти.<br>
                    Брокер-узлы отправляют дальше подзапросы к узлам обработки запросов, и когда<br>
                    результаты этих подзапросов возвращаются, брокер объединяет их и возвращает<br>
                    финальный комбинированный результат пользователю.<br>
                    <br>
                    Я не берусь предполагать, зачем при проектировании Druid и Pinot было принято<br>
                    решение о введении еще одного типа узлов. Однако, теперь они кажутся их<br>
                    неотъемлемой частью, поскольку, когда общее количество сегментов в кластере<br>
                    начинает превышать десять миллионов, информация об отображении сегментов в<br>
                    узлы начинает занимать гигабайты памяти. Это очень расточительно – выделять<br>
                    столько много памяти на каждом узле для обработки запросов. Вот вам и еще один<br>
                    недостаток, который накладывается на Druid и Pinot их «сегментированной»<br>
                    архитектурой управления данными.<br>
                    <br>
                    В <b>ClickHouse</b> выделять отдельный набор узлов под “брокер запросов” обычно не<br>
                    требуется. Существует специальный, эфемерный <a href="https://clickhouse.tech/docs/en/table_engines/distributed.html"> “распределенный” тип таблицы</a> в<br>
                    ClickHouse, который может быть установлен на любом узле, и запросы к этой таблице<br>
                    будут делать все то же, за что отвечают брокер-узлы в Druid и Pinot. Обычно подобные<br>
                    эфемерные таблицы размещаются на каждом узле, который участвует в<br>
                    секционированной таблице, так что на практике каждый узел может быть “входной<br>
                    точкой” для запроса в кластер ClickHouse. Этот узел может выпускать необходимые<br>
                    подзапросы к другим секциями, обрабатывать свою часть запроса самостоятельно и<br>
                    затем объединять её с частичными результатами от других секций.<br>
                    <br>
                    Когда узел (или один из процессинговых узлов в ClickHouse, или брокер-узел в Druid и<br>
                    Pinot) выпускает подзапросы к другим, и один или несколько подзапросов по какой-<br>
                    либо причине заканчиваются неудачей, ClickHouse и Pinot обрабатывают эту ситуацию<br>
                    правильно: они объединяют результаты успешно выполненных подзапросов вместе,<br>
                    и всё равно возвращают частичный результат пользователю. <a href="https://leventov.medium.com/the-problems-with-druid-at-large-scale-and-high-load-part-1-714d475e84c9">Druid этой функции сейчас<br>очень недостает</a>
                    : если в нем выполнение подзапроса заканчивается неудачей,<br>
                    то неудачей закончится и весь запрос целиком.
                </p>
            </div>
            <div class="ClickHouse vs. Druid или Pinot: Выводы">
                <h3>ClickHouse vs. Druid или Pinot: Выводы</h3>
                <p>
                    “Сегментированный” подход к управлению данными в Druid и Pinot против более<br>
                    простого управления данными в ClickHouse определяет многие аспекты систем.<br>
                    Однако, важно заметить, что это различие оказывает небольшое (или не оказывает<br>
                    вовсе) влияние на потенциальную эффективность сжатия (впрочем, история про<br>
                    компрессию для всех трех систем имеет печальный конец по нынешнему состоянию<br>
                    дел), или на скорость обработки запросов.<br>
                    <br>
                    <b>ClickHouse</b> похож на традиционные RDMBS, например, PostgreSQL. В частности,<br>
                    ClickHouse можно развернуть на всего один сервер. Если планируемый размер<br>
                    невелик - скажем, не больше порядка 100 ядер CPU для обработки запросов и 1 TB<br>
                    данных, я бы сказал, что ClickHouse имеет значительные преимущества перед Druid и<br>
                    Pinot в силу своей простоты и отсутствия необходимости в дополнительных типах<br>
                    узлов, таких как “мастер”, “узлы поглощения в реальном времени”, “брокеры”. На этом<br>
                    поле, ClickHouse соревнуется скорее с InfluxDB, чем с Druid или Pinot.<br>
                    <br>
                    <b>Druid and Pinot</b> похож на системы Big Data вроде HBase. Здесь в виду имеются не<br>
                    характеристики производительности, а зависимость от ZooKeper, зависимость от<br>
                    персистентного реплицируемого хранилища (к примеру, HDFS), сосредоточение<br>
                    внимания на устойчивости к отказам отдельных узлов, а также автономная работа и<br>
                    управление данными, не требующими постоянного внимания человека.<br>
                    <br>
                    Для широкого спектра приложений, ни ClickHouse, ни Druid или Pinot не являются<br>
                    очевидными победителями. В первую очередь, вы должны принимать во внимание<br>
                    вашу способность разобраться с исходным кодом системы, исправлять баги,<br>
                    добавлять новые функции и т.д. Это подробнее обсуждается в разделе “Про<br>
                    сравнение производительности и выбор системы”.<br>
                    <br>
                    Во-вторых, вам стоит взглянуть на таблицу ниже. Каждая ячейка в этой таблице<br>
                    описывает свойство приложения, которое позволит определить предпочтительную<br>
                    систему. Строки отсортированы не в порядке важности. Важность различных свойств<br>
                    может разниться от приложения к приложению, но в целом можно применить<br>
                    следующий подход: если ваше приложение соответствует подавляющему<br>
                    большинству строк со свойствами в одной из колонок, то относящаяся к ней система в<br>
                    вашем случае является предпочтительным выбором.<br>
                </p>
                <table border="1">
                    <tr>
                        <th>
                            ClickHouse
                        </th>
                        <th>
                            Druid или Pinot
                        </th>
                    </tr>
                    <tr>
                        <td>В организации есть эксперты по C++</td>
                        <td>В организации есть эксперты по Java</td>
                    </tr>
                    <tr>
                        <td>Малый кластер/td>
                        <td>Большой кластер</td>
                    </tr>
                    <tr>
                        <td>Немного таблиц</td>
                        <td>Много таблиц</td>
                    </tr>
                    <tr>
                        <td>Один набор данных</td>
                        <td>Несколько несвязанных наборов данных</td>
                    </tr>
                    <tr>
                        <td>Таблицы и данные находятся в кластере<br>перманентно</td>
                        <td>Таблицы и наборы данных периодически<br>появляются в кластере и удаляются из<br>него</td>
                    </tr>
                    <tr>
                        <td>Размер таблиц (и интенсивность<br>запросов к ним) остается стабильным во<br>времени</td>
                        <td>Таблицы значительно растут и<br>сжимаются</td>
                    </tr>
                    <tr>
                        <td>Однородные запросы (их тип, размер,<br>распределение по времени суток и т.д.)</td>
                        <td>Разнородные запросы</td>
                    </tr>
                    <tr>
                        <td>В данных есть измерение, по которому<br>оно может быть сегментировано, и почти<br>не выполняется запросов, которые<br>затрагивают данные, расположенные в<br>нескольких сегментах</td>
                        <td>Подобного измерения нет, и запросы<br>часто затрагивают данные,<br>расположенные во всем кластере</td>
                    </tr>
                    <tr>
                        <td>Облако не используется, кластер должен<br>быть развернут на специфическую конфигурацию<br>физических серверов</td>
                        <td>Кластер развернут в облаке</td>
                    </tr>
                    <tr>
                        <td>Нет существующих кластеров Hadoop<br>или Spark</td>
                        <td>Кластеры Hadoop или Spark уже<br>существуют и могут быть использованы</td>
                    </tr>
                </table>
                <p>
                    <b>Примечание:</b> ни одно из свойств выше не означает, что вы должны использовать<br>
                    соответствующую систему (системы), или избегать другую. К примеру, если<br>
                    планируется, что ваш кластер будет большим, это не значит, что вы обязательно<br>
                    должны рассматривать только Druid или Pinot, исключив ClickHouse. Скорее всего, в<br>
                    данной ситуации Druid или Pinot могут быть лучшим выбором, но другие полезные<br>
                    свойства могут перевесить чашу весов в сторону ClickHouse, который для некоторых<br>
                    приложений является оптимальным выбором даже для больших кластеров.
                </p>
            </div>
        </div>
        <div class="Различия между Druid и Pinot">
            <h2>Различия между Druid и Pinot</h2>
            <p>
                Как уже не раз отмечалось в данной статье, Druid и Pinot имеют весьма похожие<br>
                архитектуры. Есть несколько достаточно заметных особенностей, которые есть в<br>
                одной системе и отсутствуют в другой, и областей, в которых каждая из систем<br>
                развита гораздо сильнее другой. Тем не менее, всё, о чем я собираюсь упомянуть<br>
                ниже, можно воспроизвести в другой системе, приложив разумное количество усилий.<br>
                <br>
                Между Druid и Pinot существует лишь <b>одно существенное различие</b>, которое<br>
                слишком велико для того, чтобы от него избавились в обозримом будущем - это<br>
                <b>реализация управления сегментами</b> в мастер-ноде. Кстати, разработчики обеих<br>
                систем наверняка не хотели бы этого делать в любом случае, поскольку оба подхода<br>
                имеют свои “за” и “против” - среди них нет такого, который был бы лучше.
            </p>
            <div class="Управление сегментами в Druid">
                <h3>Управление сегментами в Druid</h3>
                <p>
                    Мастер-нода в Druid (и ни один из узлов в Pinot) не отвечают за сохранность<br>
                    метаданных в сегментах данных в кластере, и текущее отображение между<br>
                    сегментами и узлами обработки данных, на которых загружены сегменты. Эта<br>
                    информация хранится в ZooKeeper. Однако, Druid в дополнение хранит эту<br>
                    информацию еще и в SQL базе данных, которая необходима для развертывания<br>
                    кластера Druid. Не могу сказать, с какой целью было принято такое решение, но<br>
                    сейчас оно дает следующие преимущества:
                </p>
                <ul>
                    <li>
                        <p>
                            В ZooKeeper <b>хранится меньше данных</b>. Только минимум информации об<br>
                            отображении идентификатора сегмента на список узлов, занимающихся<br>
                            обработкой запросов, куда загружен сегмент, сохраняется в ZooKeeper.<br>
                            Оставшиеся метаданные, к примеру, размер сегмента, список измерений и<br>
                            метрики, и т.д. - хранятся только в SQL базе данных.
                        </p>
                    </li>
                    <li>
                        <p>
                            Когда сегменты данных вытесняются из кластера, поскольку они становятся<br>
                            слишком старыми (это общая функция всех баз данных временных рядов - она<br>
                            есть и в ClickHouse, и в Druid, и в Pinot), они выгружаются из узлов обработки<br>
                            запросов и их метаданные удаляются из ZooKeeper, но не из “глубокого<br>
                            хранилища” и не из базы данных SQL. Пока они не будут удалены из этих мест<br>
                            вручную, <b>остается возможность “оживить” действительно старые данные</b><br>
                            быстро, если он потребуются для построения отчетов или исследований.
                        </p>
                    </li>
                    <li>
                        <p>
                            Вряд ли это планировалось с самого начала, но теперь есть планы сделать<br>
                            <b>зависимость Druid от ZooKeeper опциональной</b>. Сейчас ZooKeeper<br>
                            используется для трех различных функций: управления сегментами,<br>
                            обнаружения сервисов и хранения свойств (например, для управления<br>
                            поглощением данных в реальном времени). Обнаружение сервисов может <a href="https://groups.google.com/forum/#!msg/druid-development/eIWDPfhpM_U/Em06lGjhAwAJ">быть<br>предоставлено Consul</a>.
                            Управление сегментами может быть реализовано <a href="https://groups.google.com/forum/#!msg/druid-development/tWnwPyL0Vk4/2uLwqgQiAAAJ">при<br>помощи HTTP-команд</a>,
                            и оно доступно нам благодаря тому, что функции<br>
                            хранения в ZooKeeper “бекапится” в базе SQL.

                        </p>
                    </li>
                </ul>
                <p>
                    То что нам приходится иметь в зависимостях базу данных SQL, приводит к большей нагрузке на эксплуатацию, особенно, если в компании еще не использовалась какая-либо БД SQL. Druid поддерживает MySQL и PostgreSQL, есть и расширение для Microsoft SQL Server. Кроме того, когда Druid рразворачивается в облаке, можно использовать стандартные сервисы для управления RDBMS - к примеру, Amazon RDS.
                </p>
            </div>
            <div class="Управление сегментами в Pinot">
                <h3>Управление сегментами в Pinot</h3>
                <p>
                    В отличие от Druid, который реализует всю логику управления сегментами<br>
                    самостоятельно и полагается только на <a href="https://curator.apache.org/">Curator</a> для взаимодействия с ZooKeeper, Pinot<br>
                    делегирует большую часть логики управления сегментами и кластерами на <a href="https://helix.apache.org/">фреймворк<br>Helix</a>.<br> 
                    <br>
                    С одной стороны, я могу понять, что это дает разработчикам Pinot возможность<br>
                    сосредоточиться на других частях их системы. В Helix возможно меньше багов, чем в<br>
                    логике внутри самого Druid, поскольку он тестируется в других условиях и поскольку в<br>
                    него, предположительно, было вложено гораздо больше рабочего времени.<br>
                    <br>
                    С другой стороны, Helix возможно ограничивает Pinot своими “рамками фреймворка”.<br>
                    Helix, и следовательно, <b>Pinot, скорее всего будут зависеть от ZooKeeper всегда</b>.<br>
                    <br>
                    Далее я собираюсь перечислить менее важные различия между Druid и Pinot - в том<br>
                    смысле, что если у вас возникнет серьезное желание повторить одну из этих функций<br>
                    в вашей системе, то это будет вполне осуществимо.
                </p>
            </div>
            <div class="«Проталкивание предикатов» в Pinot">
                <h3>«Проталкивание предикатов» в Pinot</h3>
                <p>
                    Если во время поглощения данные секционируются в Kafka по каким-либо ключам<br>
                    измерений, Pinot создает сегменты, которые содержат информацию об этом<br>
                    разбиении и затем, когда выполняется запрос с предикатом на данном измерении,<br>
                    брокер-узел фильтрует сегменты таким образом, чтобы как можно меньше сегментов<br>
                    и узлов обработки запросов было затронуто.<br>
                    <br>
                    Эта концепция в оригинале называется <b>“predicate pushdown”</b> и важна для<br>
                    поддержания высокой производительности в некоторых приложениях.<br>
                    <br>
                    На данный момент Druid поддерживает разбиение по ключам, если сегменты были<br>
                    созданы в Hadoop, но еще не поддерживает сегменты, созданные во время<br>
                    поглощения в реальном времени. Druid сейчас не реализует функцию «проталкивания<br>
                    предикатов» на брокеры.
                </p>
            </div>
            <div class="“Сменный” Druid и своевольный Pinot">
                <h3>“Сменный” Druid и своевольный Pinot</h3>
                <p>
                    Поскольку Druid используют различные организации и в его разработке принимают <br>
                    участие несколько компаний, он обзавелся поддержкой нескольких взаимозаменяемых<br>
                    опций для практически любой выделенной части или “сервиса”:
                </p>
                <ul>
                    <li>
                        HDFS, Cassandra, Amazon S3, Google Cloud Storage или Azure Blob Storage и<br>
                        т.д. в качестве “глубокого хранилища”;
                    </li>
                    <li>
                        Kafka, илиr RabbitMQ, Samza, или Flink, или Spark, Storm, и т.д. (через<br>
                        <a href="https://github.com/druid-io/tranquility">Tranquility</a> ) в качестве источника поглощения данных в реальном времени;
                    </li>
                    <li>
                        Сам Druid, или Graphite, или Ambari, или StatsD, или Kafka в качестве “слива”<br>
                        для телеметрии кластера Druid (метрик).
                    </li>
                </ul>
                <p>
                    В то же время Pinot почти целиком разрабатывался исключительно в стенах LinkedIn и<br>
                    должен был удовлетворять текущим нуждам компании, поэтому выбор, который вам<br>
                    предлагается, не так велик. В качестве “глубокого хранилища” необходимо<br>
                    использовать HDFS или Amazon S3, а для поглощения данных в реальном времени<br>
                    подойдет только Kafka. Но если кому-то это действительно понадобится, мне кажется,<br>
                    не составит особой сложности добавить поддержку любого другого сервиса в Pinot. К<br>
                    тому же, можно ожидать позитивных сдвигов в этом направлении, поскольку <a href="https://www.slideshare.net/XIANGFU3/pinot-near-realtime-analytics-uber">Uber</a> и<br>
                    Slack начинают использовать Pinot.
                </p>
            </div>
            <div class="Формат данных и движок выполнения запросов лучше оптимизированы в Pinot">
                <h3>Формат данных и движок выполнения запросов лучше оптимизированы в Pinot</h3>
                <p>
                    В частности, следующие функции <a href="https://github.com/apache/incubator-pinot/wiki#anatomy-of-index-segment">формата сегментов Pinot</a> сейчас отсутствуют в <br>
                    Druid:
                </p>
                <ul>
                    <li>
                        <b>Сжатие проиндексированных столбцов</b> с битовой гранулярностью, но<br>
                        байтовой гранулярностью в Druid.
                    </li>
                    <li>
                        <b>Инвертированный индекс опционален</b> для каждого столбца. В Druid он<br>
                        является обязательным, иногда этого не требуется, но все равно занимает<br>
                        много места. Различие в потреблении места между Druid и Pinot, <a href="https://www.slideshare.net/XIANGFU3/pinot-near-realtime-analytics-uber/17">на которое<br>указывает Uber в своих тестах</a>,
                        вполне возможно вызвано именно этим.
                    </li>
                    <li>
                        <b>Минимальные и максимальные значения</b> в числовых столбцах<br>
                        записываются посегментно.
                    </li>
                    <li>
                        <b>Поддержка сортировки данных из коробки</b>. В Druid этого можно достичь<br>
                        только вручную и слегка специфическим способом (как было описано в разделе<br>
                        “CloudFlare: ClickHouse против Druid”). Сортировка данных означает лучшее<br>
                        сжатие, и эта функция в Pinot - еще одна причина различия между Druid и Pinot<br>
                        в потреблении пространства (и производительности запросов!), на которую<br>
                        указывает Uber.
                    </li>
                    <li>
                        <b>Формат данных</b>, используемый для многозначных столбцов, на данный<br>
                        момент лучше оптимизирован в Pinot, чем в Druid.
                    </li>
                </ul>
                <p>
                    Однако, все это можно реализовать и в Druid. И несмотря на то, что формат Pinot<br>
                    оптимизирован существенно лучше, чем формат Druid, он все равно достаточно далек<br>
                    от того, чтобы быть оптимальным. Один из примеров: Pinot (как и Druid) использует<br>
                    только сжатие общего назначение (как Zstd) и еще не реализовали идеи сжатия из<br>
                    <a href="http://www.vldb.org/pvldb/vol8/p1816-teller.pdf">Gorilla</a>.<br>
                    <br>
                    К сожалению Uber по большей части использовал запросы count (*) для сравнения<br>
                    производительности Druid и Pinot относительно выполнения запроса [<a href="https://www.slideshare.net/XIANGFU3/pinot-near-realtime-analytics-uber/18">1</a>, <a href="https://www.slideshare.net/XIANGFU3/pinot-near-realtime-analytics-uber/19">2</a>], который<br>
                    сейчас в Druid представляет собой тупое линейное сканирование, хотя его и несложно<br>
                    заменить <a href="https://github.com/apache/druid/issues/4065#issuecomment-286963235">корректной O(1) реализацией</a>. Это вам еще один пример бессмысленных<br>
                    сравнений в стиле “черного ящика”, о которых мы говорили ранее.<br>
                    <br>
                    По моему мнению, причины сильного различия в производительности запросов GROUP<br>
                    BY, которое наблюдали в Uber, стоит искать в недостатке сортировки данных в<br>
                    сегментах Druid, как уже было отмечено выше в этом разделе.
                </p>
            </div>
            <div class="У Druid есть более умный алгоритм присваивания (балансировки) сегментов">
                <h3>У Druid есть более умный алгоритм присваивания (балансировки) сегментов</h3>
                <p>
                    Алгоритм Pinot заключается в присвоении сегмента к узлам обработки запроса,<br>
                    которые имеют наименьшее число сегментов, загруженных в текущий момент.<br>
                    Алгоритм Druid является гораздо более сложным; он учитывает таблицу каждого<br>
                    сегмента и время, и применяет <b>сложную формулу для вычисления финального</b><br>
                    <b>коэффициента</b>, согласно которому будут ранжированы узлы обработки запросов для<br>
                    выбора наилучшего, которому и будет присвоен новый сегмент. Этот алгоритм<br>
                    показал ускорение в скорости выполнения запросов в продакшне Metamarkets на 30-<br>
                    40%. Хотя, даже несмотря на подобный результат, мы им по-прежнему не слишком<br>
                    довольны - подробности можно прочитать <a href="https://metamarkets.com/2016/distributing-data-in-druid-at-petabyte-scale/">в отдельной статье</a>.<br>
                    <br>
                    Не знаю, как в LinkedIn управляются со всем при помощи настолько простого<br>
                    алгоритма балансировки сегментов в Pinot, но, вполне возможно, их ожидают<br>
                    значительные улучшения по части производительности, если они решатся потратить<br>
                    время на совершенствование используемого ими алгоритма.
                </p>
            </div>
            <div class="Pinot более устойчив к отказам при выполнении сложных запросов">
                <h3>Pinot более устойчив к отказам при выполнении сложных запросов</h3>
                <p>
                    Как уже упоминалось выше в разделе “Выполнение запроса”, когда брокер-узел<br>
                    создает подзапросы к другим узлам, некоторые подзапросы заканчиваются ошибкой,<br>
                    но Pinot объединяет результаты всех удачно выполненных подзапросов и по-<br>
                    прежнему возвращает частичный результат пользователю.<br>
                    <br>
                    В Druid такой функции на данный момент.

                </p>
            </div>
            <div class="Иерархия узлов обработки запросов в Druid">
                <h3>Иерархия узлов обработки запросов в Druid</h3>
                <p>
                    Смотрите аналогичный раздел выше. Druid позволяет вводить уровни узлов обработки<br>
                    запросов для старых и новых данных, и для узлов со “старыми” данными соотношение<br>
                    “ресурсы CPU, RAM / число загруженных сегментов” гораздо ниже, что позволяет<br>
                    выиграть на расходах на инфраструктуру в обмен на низкую производительность<br>
                    запросов при доступе к старым данным.<br>
                    <br>
                    Насколько мне известно, в Pinot на данный момент аналогичная функциональность<br>
                    отсутствует.                     
                </p>
            </div>
        </div>
        <div class="Заключение">
            <h2>Заключение</h2>
            <p>
                <b>ClickHouse, Druid и Pinot имеют фундаментально схожую архитектуру</b>, и <br>
                занимают свою собственную нишу между Big Data-фреймворками общего назначения<br>
                вроде Impala, Presto, Spark, и колоночными базами данных с корректной поддержкой<br>
                первичных ключей, точечных обновлений и удалений, как InfluxDB.<br>
                <br>
                В силу схожести архитектур, ClickHouse, Druid и Pinot имеют примерно одинаковый<br>
                “предел оптимизации”. Но в своем текущем состоянии, <b>все три системы еще</b><br>
                <b>незрелы</b> и очень далеки от этого лимита. Существенных улучшений в<br>
                производительности данных систем (применительно к специфическим сценариям<br>
                использования) можно достичь несколькими человеко-месяцами работы опытных<br>
                инженеров
            </p>
            <blockquote style="background-color: yellow; display: inline-block;">
                Я бы не рекомендовал вам сравнивать производительность данных систем<br>
                между собой - выберите для себя ту, чей исходный код вы способны понять и<br>
                модифицировать, или ту, в которую вы хотите инвестировать свои ресурсы.
            </blockquote>
            <p>
                Из этих трех систем, ClickHouse стоит немного в стороне от Druid и Pinot - в то время<br>
                как Druid и Pinot практически идентичны, и их можно считать двумя независимо<br>
                разрабатываемыми реализациями одной и той же системы.<br>
                <br>
                ClickHouse больше напоминает “традиционные” базы данных вроде PostgreSQL.<br>
                ClickHouse можно установить на один узел. При малых масштабах (менее 1 TB<br>
                памяти, менее 100 ядер CPU), ClickHouse выглядит гораздо более интересным<br>
                вариантом, чем Druid или Pinot - если вам все еще хочется их сравнивать - в силу того,<br>
                что ClickHouse проще и имеет меньше движущихся частей и сервисов. Я бы даже<br>
                сказал, что на таком масштабе он скорее становится конкурентом для InfluxDB или<br>
                Prometheus, а не для Druid или Pinot.<br>
                <br>
                Druid и Pinot больше напоминают другие системы Big Data из экосистемы Hadoop. Они<br>
                сохраняют свои “самоуправляемые” свойства даже на очень больших масштабах<br>
                (более 500 узлов), в то время как ClickHouse потребует для этого достаточно много<br>
                работы профессиональных SRE. Кроме того, Druid и Pinot занимают выигрышную<br>
                позицию в плане оптимизации инфраструктурной стоимости больших кластеров, и<br>
                лучше подходят для облачных окружений, чем ClickHouse.<br>
                <br>
                Единственным долгосрочным различием между Druid и Pinot является то, что <br>
                зависит от фреймворка Helix и будет продолжать зависеть от ZooKeeper, в то время<br>
                как Druid может уйти от зависимости от ZooKeeper. С другой стороны, установка Druid<br>
                продолжит зависеть от наличия какой-либо SQL-базы данных. На данный момент,<br>
                Pinot оптимизирован лучше, чем Druid.
            </p>
            <blockquote style="background-color: yellow; display: inline-block;">
                Если вы уже сталкивались с необходимостью сравнения этих систем и сделали<br>
                свой выбор, то приходите на одну из наших конференций и расскажите о своем<br>
                кейсе: о том какие именно были задачи и какие грабли (а наверняка они были) вы<br>
                встретили. Хотя, конечно, базы данных далеко не единственная тема.<br>
                Ближайший по окончанию срока подачи заявок (<b>до 9 апреля</b>) фестиваль <a href="https://ritfest.ru/2020/">РИТ++</a><br>
                включает направления: <a href="https://frontendconf.ru/">фронтенд</a>, <a href="http://backendconf.ru/moscow-rit/2019">бэкенд</a>, <a href="https://devopsconf.io/">эксплуатацию</a> и <a href="http://whalerider.ru/moscow-rit/2019">управление</a>. Участникам<br>
                обычно интереснее всего узнать о конкретных примерах, но и выступления и<br>
                в виде обзоров и исследований тоже возможны – главное, чтобы<br>
                тема была интересна лично вам.
            </blockquote>
        </div>
    </div>
</body>
</html>